---
title: "Comparing benchmarking experiment results from before and after Release 2.6"
author: "Gyuri Barab√°s"
format:
  pdf:
    keep-tex: false
    fontsize: 11pt
    documentclass: article
    papersize: a4paper
    margin-left: 1in
    margin-right: 1in
    margin-top: 1in
    margin-bottom: 1in
    code-block-border-left: false
    number-sections: false
  html:
    theme: cosmo
    number-sections: false
    embed-resources: true
    code-fold: true
knitr:
  opts_chunk:
    eval: true
    echo: false
    message: false
    warning: false
    fig-align: center
    tbl-cap-location: bottom
    R.options:
      knitr.kable.NA: ""
execute: 
  cache: false
editor_options: 
  chunk_output_type: console
latex-tinytex: false
---



```{r}
library(tidyverse) # Efficient data manipulation and plotting
```

## Introduction

This report analyzes and compares the results of the latest two benchmarking experiments. The first was run in June 2025, just before Release 2.6 went out. We will call this the `2025-06` experiment. The other one was run in July 2025 just after (the `2025-07` experiment). The comparison of `2025-06` and `2025-07` ought to reveal whether any of the download and upload speeds were affected by the new release, and how.

We begin by a recap of the experimental design. The purpose is to measure up- and download speeds on Swarm for various file sizes, erasure settings, and retrieval strategies. We vary all parameters in a fully factorial way, to get at all their possible combinations. The factors are as follows:

-   `size`: The size of the uploaded random file. We have 6 distinct factor levels: 1 KB, 10 KB, 100 KB, 1 MB, 10 MB, and 50 MB. Every file has a size that matches one of these values exactly. Importantly, every single upload is a unique random file, even if the file sizes are otherwise equal---this removes the confounding effects of caching.
-   `erasure`: The strength of erasure coding. We have five factor levels: `0` (= `NONE`), `1` (= `MEDIUM`), `2` (= `STRONG`), `3` (= `INSANE`), and `4` (= `PARANOID`).
-   `strategy`: The retrieval strategy used to download the file. Its value is necessarily `NONE` in the absence of erasure coding---i.e., when `erasure = 0`. Otherwise, it is either `DATA` or `RACE`.
-   `server`: The identity of the server initiating the downloads might influence download speeds. For a fair comparison, servers should be identical within an experiment, but it makes sense to perform the whole experimental suite over multiple different servers, to control for server-specific effects. This means that we have an extra experimental factor, with as many distinct levels as the number of distinct servers used. Here we use three distinct servers: `Server 1`, `Server 2`, and `Server 3`.
-   `replicate`: To gain sufficient sample sizes for proper statistical analysis, every single combination of the above factors is replicated 30 times. For example, given the unique combination of 1MB files uploaded without erasure coding on Server 1, we actually up- and downloaded at least 30 such files (each being a unique random file).

The above design leads to (6 filesizes) $\times$ (5 erasure code levels) $times$ (3 retrieval strategies) $\times$ (3 servers) $times$ (30 replicates) = 8100 unique download experiments. Actually, it's fewer than that, because the `NONE` retrieval strategy is only ever used when `erasure` is `0`, and the `DATA` and `RACE` strategies only when `erasure` is not `0`. So the actual number is 4860.

One slight (and useful) deviation from the above protocol was that the `2025-07` experiment used 60 replicates per parameter combination, instead of 30. So the number of individual downloads was 9720 for the most recent run.

Additionally, here are some further notes and points about the experimental design outlined above:

-   All uploads are direct, as opposed to deferred.
-   We need to make sure that no download starts after the system has properly stored the data. Since our files are relatively small, uploading should be done in a few minutes at most (as we will see later, the longest upload in our data took below 3 minutes). So we opted for a crude but reliable way of eliminating any syncing issues: we waited exactly 2 hours after every upload, and began downloading only then.
-   All downloads are done using nodes with an active checkbook.
-   Every download is re-attempted in case of a failure. In total, 15 attempts are made before giving up and declaring that the file cannot be retrieved.


## Preliminary analysis

```{r}
readDownloadData <- function(file) {
  read_rds(file) |>
    # Remove failed downloads:
    filter(sha256_match) |>
    # Remove columns that are no longer needed:
    select(!sha256_match & !attempts) |>
    # Convert erasure and strategy to factors, for easier handling later:
    mutate(erasure = fct_relevel(erasure, "NONE", "MEDIUM", "STRONG",
                                 "INSANE", "PARANOID")) |>
    mutate(strategy = fct_relevel(strategy, "NONE", "DATA", "RACE")) |>
    # Keep rows in a logical order:
    arrange(platform, erasure, strategy, size_kb, server)
}


dat <- bind_rows(
  readDownloadData("../swarm-2025-06/swarm.rds") |>
    mutate(dataset = "2025-06", .before = 1),
  readDownloadData("../swarm-2025-07/swarm.rds") |>
    mutate(dataset = "2025-07", .before = 1)
) |>
  select(!platform)
```

We can visually compare download times across the April (2025-04) and June (2025-06, that just finished yesterday) runs. We do this both for the `DATA` (@fig-compare-data) and `RACE` (@fig-compare-race) retrieval strategies.

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 9
#| fig-cap: Comparison of the April (blue, 2025-04) and June (yellow, 2025-06) benchmarking experiments, for the `DATA` retrieval strategy. Box plots are standard except no outliers are shown---that is, the thick horizontal line is the median (point that separates the top and bottom half of the data), the box around it encompasses the middle 50% of all data points, and the top/bottom whiskers show where the top/bottom 25% of the data are.
#| label: fig-compare-data


# Side-by-side visualization of the different sets of results:
compareTimePlot <- function(data, strategy) {
  data |>
    filter(strategy %in% c("NONE", {{strategy}})) |>
    ggplot(aes(x = size_kb, y = time_sec, color = dataset, fill = dataset,
               group = str_c(server, erasure, size_kb, dataset))) +
    geom_boxplot(alpha = 0.3, coef = Inf) +
    scale_x_log10(breaks = c(10, 1000, 100000),
                  labels = c("10 KB", "1 MB", "100 MB")) +
    scale_y_log10(breaks = c(0.5, 30, 1800),
                  labels = c("0.5 s", "1 m", "30 m")) +
    scale_color_manual(values = c("steelblue", "goldenrod")) +
    scale_fill_manual(values = c("steelblue", "goldenrod")) +
    labs(x = "File size", y = "Download time",
         color = "Dataset: ", fill = "Dataset: ") +
    facet_grid(erasure ~ server) +
    theme_bw() +
    theme(legend.position = "bottom")
}

compareTimePlot(dat, strategy = "DATA")
```

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 9
#| fig-cap: As @fig-compare-data, but for the `RACE` retrieval strategy.
#| label: fig-compare-race


compareTimePlot(dat, strategy = "RACE")
```

The results from the two runs look quite similar, though we appear to see consistently faster download times under the `RACE` retrieval strategy for the June experiment. However, it is unlikely that this observed difference is significant. Indeed, if we make a group-by-group comparison of the two experiments (taking @fig-compare-data and @fig-compare-race, and for each file size category / retrieval strategy / erasure coding combination, compare the April and June data using a non-parametric Wilcoxon rank sum test), none of the results come out as significant at the $\alpha = 0.05$ level, after correcting for multiple comparisons (@fig-wilcox).

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 9
#| fig-cap: Group-by-group comparison of results from the April (2025-04) and June (2025-06) benchmarking experiments. The y-axis shows the estimated difference (point) plus/minus 95% confidence intervals (error bars) from a Wilcoxon rank sum test applied to each distinct file size / retrieval strategy / erasure coding combination. Colors indicate whether the difference was found significant at the $\alpha = 0.05$ level, after false discovery rate correction to multiple testing. Since no results turned out as significant, everything is gray and no results are in blue.
#| label: fig-wilcox


dat |>
  mutate(ztime = (time_sec - mean(time_sec)) / sd(time_sec),
         .by = c(dataset, size_kb, server, erasure, strategy)) |>
  select(!time_sec) |>
  nest(data = dataset | server | ztime) |>
  filter(map_lgl(data, \(x) nrow(distinct(x, dataset)) == 2L)) |>
  mutate(wilcox = map(data, \(x) wilcox.test(ztime ~ dataset, data = x,
                                             conf.int = TRUE, conf.level = 0.95))) |>
  mutate(wilcox = map(wilcox, broom::tidy)) |>
  unnest(wilcox) |>
  select(!data & !statistic & !method & !alternative) |>
  mutate(adj.p.value = p.adjust(p.value, "fdr"), .after = p.value) |>
  mutate(signif = ifelse(adj.p.value < 0.05, "significant", "nonsignificant")) |>
  mutate(strategy = ifelse(strategy == "RACE", "RACE", "NONE/DATA")) |>
  mutate(size_kb = as_factor(size_kb)) |>
  ggplot(aes(x = size_kb, y = estimate, ymin = conf.low, ymax = conf.high,
             color = signif)) +
  geom_point() +
  geom_errorbar(width = 0.2) +
  scale_color_manual(values = c("significant" = "steelblue",
                                "nonsignificant" = "gray")) +
  facet_grid(erasure ~ strategy) +
  labs(x = "File size (KB)", y = "Estimated difference between April and June results",
       color = NULL) +
  theme_bw()
```

We can also check how much advantage the `RACE` strategy offers over `DATA`, for each file size category. The results are qualitatively similar for the April (@fig-erasure-box-04) as for the June (@fig-erasure-box-06) runs. The one difference is in the largest file size categories, which are difficult to compare because in April that size was 100 MB and in June 50 MB. That said, unless there is a sudden jump in behavior somewhere between those two file sizes, the June results look much better than the April ones did.

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 6
#| fig-cap: Download times (y-axis) as a function of erasure level (x-axis), file size (panels), and retrieval strategy (colors). Data are only for the April (2025-04) experiment. The y-axis is individually scaled for each panel. High outliers (points more than 1.5 times the interquartile range outside the box in the upper direction) have been removed, because they otherwise distort the plots and make the corresponding results difficult to compare visually.
#| label: fig-erasure-box-04


dat |>
  filter(dataset == "2025-04") |>
  mutate(strategy = ifelse(strategy != "RACE", "NONE/DATA", "RACE")) |>
  mutate(size = case_match(size_kb, 1~"1 KB", 10~"10 KB", 100~"100 KB",
                           1000~"1 MB", 10000~"10 MB", 100000~"100 MB")) |>
  mutate(size = as_factor(size)) |>
  select(size, size_kb, strategy, server, erasure, time_sec) |>
  arrange(size, strategy, erasure, server) |>
  mutate(strategy = str_remove(strategy, "NONE/")) |>
  mutate(strategy = as_factor(ifelse(erasure == "NONE", "NONE", strategy))) |>
  mutate(size = as_factor(str_c("Size: ", size))) |>
  # Remove extreme outliers, because they otherwise distort the plot:
  filter(time_sec < 1.5 * IQR(time_sec) + quantile(time_sec, 0.75),
         .by = c(size, strategy, erasure)) |>
  ggplot(aes(x = erasure, y = time_sec, color = strategy, fill = strategy)) +
  geom_boxplot(alpha = 0.3, coef = Inf) +
  labs(color = "Strategy: ", fill = "Strategy: ", x = "Erasure level",
       y = "Download time (seconds)") +
  facet_wrap(~ size, scales = "free_y") +
  scale_color_manual(values = c("plum3", "steelblue", "goldenrod")) +
  scale_fill_manual(values = c("plum3", "steelblue", "goldenrod")) +
  theme_bw() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 40, vjust = 0.75, hjust = 0.6))
```

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 6
#| fig-cap: As @fig-erasure-box-04, but for the June (2025-06) experiment.
#| label: fig-erasure-box-06


dat |>
  filter(dataset == "2025-06") |>
  mutate(strategy = ifelse(strategy != "RACE", "NONE/DATA", "RACE")) |>
  mutate(size = case_match(size_kb, 1~"1 KB", 10~"10 KB", 100~"100 KB",
                           1000~"1 MB", 10000~"10 MB", 50000~"50 MB")) |>
  mutate(size = as_factor(size)) |>
  select(size, size_kb, strategy, server, erasure, time_sec) |>
  arrange(size, strategy, erasure, server) |>
  mutate(strategy = str_remove(strategy, "NONE/")) |>
  mutate(strategy = as_factor(ifelse(erasure == "NONE", "NONE", strategy))) |>
  mutate(size = as_factor(str_c("Size: ", size))) |>
  # Remove extreme outliers, because they otherwise distort the plot:
  filter(time_sec < 1.5 * IQR(time_sec) + quantile(time_sec, 0.75),
         .by = c(size, strategy, erasure)) |>
  ggplot(aes(x = erasure, y = time_sec, color = strategy, fill = strategy)) +
  geom_boxplot(alpha = 0.3, coef = Inf) +
  labs(color = "Strategy: ", fill = "Strategy: ", x = "Erasure level",
       y = "Download time (seconds)") +
  facet_wrap(~ size, scales = "free_y") +
  scale_color_manual(values = c("plum3", "steelblue", "goldenrod")) +
  scale_fill_manual(values = c("plum3", "steelblue", "goldenrod")) +
  theme_bw() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 40, vjust = 0.75, hjust = 0.6))
```

When looking at upload speeds, they are nearly the same across the April and June runs, but June is slightly faster (@fig-uploads).

```{r}
uploadDataFromJsonRaw <- function(jsonFile) {
  jsonlite::fromJSON(jsonFile) |>
    (`[`)(1) |>
    as_tibble() |>
    unnest(swarm) |>
    rename(erasure = ul_redundancy, time_sec = upload_time)
}


uploadFileSizeFromJsonRaw <- function(jsonFile) {
  jsonlite::fromJSON(jsonFile) |>
    (`[`)(1) |>
    (`[[`)(1) |>
    names() |>
    (\(x) if (length(x) > 0) as.integer(x) else NA_integer_)()
}


correctedSize <- function(erasure, encryption) {
  case_when( # File size overhead from erasure coding and packed-address chunks
    (erasure == "NONE")     & (encryption == "unencrypted") ~ (128/128) * (128/127),
    (erasure == "MEDIUM")   & (encryption == "unencrypted") ~ (128/119) * (128/127),
    (erasure == "STRONG")   & (encryption == "unencrypted") ~ (128/107) * (128/127),
    (erasure == "INSANE")   & (encryption == "unencrypted") ~ (128/97)  * (128/127),
    (erasure == "PARANOID") & (encryption == "unencrypted") ~ (128/38)  * (128/127),
    (erasure == "NONE")     & (encryption == "encrypted")   ~ (64/64)   * (64/63),
    (erasure == "MEDIUM")   & (encryption == "encrypted")   ~ (64/59)   * (64/63),
    (erasure == "STRONG")   & (encryption == "encrypted")   ~ (64/53)   * (64/63),
    (erasure == "INSANE")   & (encryption == "encrypted")   ~ (64/48)   * (64/63),
    (erasure == "PARANOID") & (encryption == "encrypted")   ~ (64/19)   * (64/63)
  )
}


dataFromRefFiles <- function(dataset) {
  tibble(file = Sys.glob(str_c("../swarm-", dataset, "/references/*"))) |>
  mutate(size_kb = map_int(file, uploadFileSizeFromJsonRaw)) |>
  mutate(data = map(file, uploadDataFromJsonRaw)) |>
  unnest(data) |>
  select(erasure, size_kb, time_sec) |>
  arrange(erasure, size_kb, time_sec) |>
  mutate(erasure = as_factor(case_match(
    erasure,
    0 ~ "NONE",
    1 ~ "MEDIUM",
    2 ~ "STRONG",
    3 ~ "INSANE",
    4 ~ "PARANOID"
  )))
}


datUpload <-
  tibble(dataset = c("2025-04", "2025-06")) |>
  mutate(data = map(dataset, dataFromRefFiles)) |>
  unnest(data)
```

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 5
#| fig-cap: Upload times in the April (left panel) and June (right panel) benchmarking experiments. The graphs take into account the effective sizes of the files---that is, overhead from erasure coding and packed-address chunks have been accounted for.
#| label: fig-uploads


datUpload |>
  mutate(eff_size_kb = size_kb * correctedSize(erasure, "unencrypted")) |>
  ggplot(aes(x = eff_size_kb, color = erasure, fill = erasure)) +
  geom_boxplot(aes(y = time_sec, group = str_c(erasure, eff_size_kb)),
               alpha = 0.3, width = 0.2) +
  scale_x_log10(breaks = c(10, 1000, 100000),
                labels = c("10 KB", "1 MB", "100 MB")) +
  scale_y_log10(breaks = c(0.5, 30, 1800),
                labels = c("0.5 s", "1 m", "30 m")) +
  scale_color_viridis_d(option = "C", end = 0.85) +
  scale_fill_viridis_d(option = "C", end = 0.85) +
  labs(x = "Effective file size", y = "Upload time",
       color = "Erasure coding: ", fill = "Erasure coding: ") +
  facet_grid(. ~ dataset) +
  theme_bw() +
  theme(legend.position = "bottom")
```

Since the two experiments were run under the same release version, the differences between them are caused purely by the network, and its number and distribution of nodes.
