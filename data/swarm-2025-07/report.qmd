---
title: "Comparing benchmarking experiment results from before and after Release 2.6"
author: "Gyuri Barab√°s"
format:
  pdf:
    keep-tex: false
    fontsize: 11pt
    documentclass: article
    papersize: a4paper
    margin-left: 1in
    margin-right: 1in
    margin-top: 1in
    margin-bottom: 1in
    code-block-border-left: false
    number-sections: false
  html:
    theme: cosmo
    number-sections: false
    embed-resources: true
    code-fold: true
knitr:
  opts_chunk:
    eval: true
    echo: false
    message: false
    warning: false
    fig-align: center
    tbl-cap-location: bottom
    R.options:
      knitr.kable.NA: ""
execute: 
  cache: false
editor_options: 
  chunk_output_type: console
latex-tinytex: false
---



```{r}
library(jsonlite) # Importing JSON files and making them into tables
library(tidyverse) # Efficient data manipulation and plotting
library(ggbeeswarm) # Plotting data points resembling underlying distribution
```



## Introduction

This report analyzes and compares the results of the latest two benchmarking experiments. The first was run in June 2025, just before Release 2.6 went out. We will call this the pre-release experiment. The other one was run in July 2025 just after (the post-release experiment). The comparison of the pre- and post-release experiments ought to reveal whether any of the download and upload speeds were affected by the new release, and how.

We begin by a recap of the experimental design. The purpose is to measure up- and download speeds on Swarm for various file sizes, erasure settings, and retrieval strategies. We vary all parameters in a fully factorial way, to get at all their possible combinations. The factors are as follows:

-   `size`: The size of the uploaded random file. We have 6 distinct factor levels: 1 KB, 10 KB, 100 KB, 1 MB, 10 MB, and 50 MB. Every file has a size that matches one of these values exactly. Importantly, every single upload is a unique random file, even if the file sizes are otherwise equal---this removes the confounding effects of caching.
-   `erasure`: The strength of erasure coding. We have five factor levels: `0` (= `NONE`), `1` (= `MEDIUM`), `2` (= `STRONG`), `3` (= `INSANE`), and `4` (= `PARANOID`).
-   `strategy`: The retrieval strategy used to download the file. Its value is necessarily `NONE` in the absence of erasure coding---i.e., when `erasure = 0`. Otherwise, it is either `DATA` or `RACE`.
-   `server`: The identity of the server initiating the downloads might influence download speeds. For a fair comparison, servers should be identical within an experiment, but it makes sense to perform the whole experimental suite over multiple different servers, to control for server-specific effects. This means that we have an extra experimental factor, with as many distinct levels as the number of distinct servers used. Here we use three distinct servers: `Server 1`, `Server 2`, and `Server 3`.
-   `replicate`: To gain sufficient sample sizes for proper statistical analysis, every single combination of the above factors is replicated 30 times. For example, given the unique combination of 1MB files uploaded without erasure coding on Server 1, we actually up- and downloaded at least 30 such files (each being a unique random file).

The above design has (6 file sizes) $\times$ (5 erasure code levels) $\times$ (3 retrieval strategies) $\times$ (3 servers) $\times$ (30 replicates). However, the `NONE` retrieval strategy is only ever used when `erasure` is `NONE`, and the `DATA` and `RACE` strategies only when `erasure` is not `NONE`. So the total number of unique download experiments is (30 replicates) $\times$ (6 file sizes) $\times$ (3 servers) $\times$ (1 strategy & erasure level + 2 strategies $\times$ 4 erasure levels), or $4860$.

Additionally, here are some further notes about the experimental design outlined above:

-   All uploads are direct, as opposed to deferred.
-   We need to make sure that no download starts after the system has properly stored the data. Since our files are relatively small, uploading should be done in a few minutes at most (as we will see later, the longest upload in our data took below 3 minutes). So we opted for a crude but reliable way of eliminating any syncing issues: we waited exactly 2 hours after every upload, and began downloading only then.
-   All downloads are done using nodes with an active chequebook.
-   Every download is re-attempted in case of a failure. In total, 15 attempts are made before giving up and declaring that the file cannot be retrieved.



## Download times

Both experiments uploaded all files without problems. The same holds for downloads, except for 6 failed attempts in the post-release experiment. These failed download attempts were all for 50 MB files with `STRONG` erasure coding and the `DATA` retrieval strategy, distributed evenly across the three servers (i.e., two fails per each). These six entries were removed from the data before the analysis. This leaves 9714 downloads in our dataset: 4860 per each of the two experiments, minus the 6 failed ones.

```{r}
readDownloadData <- function(file) {
  read_rds(file) |>
    # Remove failed downloads:
    filter(sha256_match) |>
    # Remove columns that are no longer needed:
    select(!sha256_match & !attempts) |>
    # Convert erasure & strategy to factors, for easier handling later:
    mutate(erasure = fct_relevel(erasure, "NONE", "MEDIUM", "STRONG",
                                 "INSANE", "PARANOID")) |>
    mutate(strategy = fct_relevel(strategy, "NONE", "DATA", "RACE")) |>
    # Keep rows in a logical order:
    arrange(platform, erasure, strategy, size_kb, server, time_sec)
}


humanReadableSize <- function(size_kb) {
  gdata::humanReadable(1000*size_kb, standard = "SI", digits = 0)
}


# Side-by-side visualization of the different sets of results:
compareTimePlot <- function(data, strat) {
  data |>
    filter(as.character(strategy) %in% c("NONE", strat)) |>
    mutate(size_b = 1000*size_kb) |>
    ggplot(aes(x = size_b, y = time_sec, color = dataset,
               group = str_c(server, erasure, size_b, dataset))) +
    geom_quasirandom(alpha = 0.3, dodge.width = 0.6) +
    scale_x_log10(labels = scales::label_bytes()) +
    scale_y_log10() +
    scale_color_manual(values = c("steelblue", "goldenrod")) +
    labs(x = "File size", y = "Download time (seconds)",
         color = "Dataset: ", fill = "Dataset: ") +
    facet_grid(erasure ~ server) +
    guides(color = guide_legend(override.aes = list(alpha = 1))) +
    theme_bw() +
    theme(legend.position = "bottom")
}


# Compare download time z-scores visually:
compareZplot <- function(data, strat) {
  data |>
    mutate(ztime = (time_sec - mean(time_sec)) / sd(time_sec),
           .by = c(size_kb, server, erasure, strategy)) |>
    mutate(size = str_trim(humanReadableSize(size_kb))) |>
    filter(as.character(strategy) %in% c("NONE", strat)) |>
    mutate(size = as_factor(size)) |>
    ggplot(aes(x = size, y = ztime, color = dataset, fill = dataset,
               group = str_c(server, erasure, size, dataset))) +
    geom_quasirandom(alpha = 0.3, dodge.width = 0.8) +
    scale_color_manual(values = c("steelblue", "goldenrod")) +
    labs(x = "File size", y = "Download time (z-score)",
         color = "Dataset: ", fill = "Dataset: ") +
    facet_grid(erasure ~ server) +
    guides(color = guide_legend(override.aes = list(alpha = 1))) +
    theme_bw() +
    theme(legend.position = "bottom")
}



dat <- bind_rows(
  readDownloadData("../swarm-2025-06/swarm.rds") |>
    mutate(dataset = "Pre-release", .before = 1),
  readDownloadData("../swarm-2025-07/swarm.rds") |>
    mutate(dataset = "Post-release", .before = 1)
) |>
  select(!platform) |>
  mutate(dataset = fct_relevel(dataset, "Pre-release"))
```

We can visually compare download times across the pre-release and post-release benchmarking runs. We do this both for the `DATA` (@fig-compare-data) and `RACE` (@fig-compare-race) retrieval strategies. For smaller file sizes (especially in the absence of erasure coding), the new release appears to be faster, but for large files it looks clearly slower. To facilitate the comparison, it helps to bring the download times, which vary over several orders of magnitude depending on file size, to the same scale. One can do this by z-transforming download times: for data points within each combination of file size category, server identity, erasure level, and retrieval strategy, we subtract the mean from the download times, and divide this difference by their standard deviation:
$$
z_i = \frac{t_i - \bar{t}}{\text{sd}(t)} ,
$$ {#eq-z}
where $t_i$ is the $i$th measured download time (in seconds), $\bar{t}$ is their mean, and $\text{sd}(t)$ their standard deviation. The resulting z-scores, $z_i$, are unitless, and have by definition mean zero and variance one. This makes them much easier to compare visually across various experimental factor combinations (@fig-z-data, @fig-z-race).

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 9
#| fig-cap: Comparison of the pre-release (blue) and post-release (yellow) benchmarking experiments, for the `DATA` retrieval strategy. Panel rows are erasure levels, panel columns are the three servers, the x-axis is file size, and the y-axis is download time in seconds (both are on the log scale). Each point is one download. Points are arranged to reflect their underlying distribution.
#| label: fig-compare-data

compareTimePlot(dat, "DATA")
```

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 9
#| fig-cap: As @fig-compare-data, but for the `RACE` retrieval strategy.
#| label: fig-compare-race

compareTimePlot(dat, "RACE")
```

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 9
#| fig-cap: As @fig-compare-data, with z-scores along the y-axis.
#| label: fig-z-data

compareZplot(dat, "DATA")
```

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 9
#| fig-cap: As @fig-z-data, but for the `RACE` retrieval strategy.
#| label: fig-z-race

compareZplot(dat, "RACE")
```

The z-scores do not change the interpretation of the original figures much, but do highlight that the speed gains for small file sizes, while probably real, are not particularly interesting. To rigorously compare the pre- and post-release results within each file size, server, erasure level, and retrieval strategy category, we can perform Wilcoxon rank sum tests.^[It does not matter whether the tests are performed on the z-scores or the original download times, because the Wilcoxon rank sum test only cares about the ranks of values (i.e., whether a value is the largest, second largest, etc.) and not the values themselves. The transformation of @eq-z preserves the order of ranks. Below we opt to do the test with the original data, to make the resulting estimates correspond to the actual download times as opposed to the z-transformed times.] Since we are performing a large number of these tests, $p$-values should be corrected for multiple comparisons. Here we do this using the false discovery rate method. The results of these statistical tests are in @fig-wilcox-download. We see that our original intuitions are confirmed. Despite statistical significance, for file sizes below 50 MB the observed differences are unlikely to make a practical difference to user experience. However, the slowing down for 50 MB files is more substantial, given that the new release loses between 10 and 30 seconds in a download process that typically takes between half a minute to a minute.

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 7
#| fig-cap: Group-by-group comparison of results from the pre- and post-release benchmarking experiments. The y-axis shows the estimated difference (point) plus/minus 95% confidence intervals (error bars) from a Wilcoxon rank sum test applied to each distinct file size / retrieval strategy / erasure coding combination. Results favoring the post-release test with $p < 0.05$ (after false discovery rate correction) are in blue, those favoring the pre-release test are in red, and those with $p > 0.05$ are in grey.
#| label: fig-wilcox-download

dat |>
  mutate(size = fct_reorder(
    str_trim(humanReadableSize(size_kb)), size_kb
  )) |>
  select(!size_kb) |>
  nest(data = dataset | server | time_sec) |>
  mutate(wilcox = map(data, \(x) {
    wilcox.test(time_sec ~ dataset, data = x,
                conf.int = TRUE, conf.level = 0.95)
  } )) |>
  mutate(wilcox = map(wilcox, broom::tidy)) |>
  unnest(wilcox) |>
  select(!data & !statistic & !method & !alternative) |>
  mutate(adj.p.value = p.adjust(p.value, "fdr"), .after = p.value) |>
  mutate(adv = case_when(
    adj.p.value <  0.05 & estimate > 0 ~ "Post-release",
    adj.p.value <  0.05 & estimate < 0 ~ "Pre-release",
    adj.p.value >= 0.05                ~ "No difference"
  )) |>
  mutate(adv = fct_relevel(adv, "Post-release", "No difference")) |>
  mutate(strategy = ifelse(strategy == "RACE", "RACE", "NONE/DATA")) |>
  ggplot() +
  geom_hline(yintercept = 0, alpha = 0.4, linetype = "dashed") +
  geom_point(aes(x = strategy, y = estimate, color = adv)) +
  geom_errorbar(aes(x = strategy, ymin = conf.low, ymax = conf.high,
                    color = adv), width = 0.2) +
  scale_color_manual(
    name = "Speed advantage:",
    values = c("Post-release"  = "steelblue",
               "No difference" = "grey70",
               "Pre-release"   = "firebrick")
  ) +
  facet_grid(size ~ erasure, scales = "free_y") +
  labs(x = NULL, y = "Estimated difference (seconds)") +
  theme_bw() +
  theme(legend.position = "bottom")
```

We can also check how much advantage the `RACE` strategy offers over `DATA`, for each file size category. The results are qualitatively similar for the pre-release (@fig-erasure-box-pre) as for the post-release (@fig-erasure-box-post) runs.

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 6
#| fig-cap: Download times (y-axis) as a function of erasure level (x-axis), file size (panels), and retrieval strategy (colors). Data are only for the pre-release experiment. The y-axis is individually scaled for each panel. High outliers (points more than 1.5 times the interquartile range outside the box in the upper direction) have been removed, because they otherwise distort the plots and make the corresponding results difficult to compare visually.
#| label: fig-erasure-box-pre


dat |>
  filter(dataset == "Pre-release") |>
  mutate(strategy = ifelse(strategy != "RACE", "NONE/DATA", "RACE")) |>
  mutate(size = humanReadableSize(size_kb)) |>
  mutate(size = as_factor(size)) |>
  select(size, size_kb, strategy, server, erasure, time_sec) |>
  arrange(size, strategy, erasure, server) |>
  mutate(strategy = str_remove(strategy, "NONE/")) |>
  mutate(strategy = as_factor(ifelse(erasure == "NONE", "NONE", strategy))) |>
  mutate(size = as_factor(str_c("Size: ", size))) |>
  # Remove extreme outliers, because they otherwise distort the plot:
  filter(time_sec < 1.5 * IQR(time_sec) + quantile(time_sec, 0.75),
         .by = c(size, strategy, erasure)) |>
  ggplot(aes(x = erasure, y = time_sec, color = strategy, fill = strategy)) +
  geom_boxplot(alpha = 0.3, coef = Inf) +
  labs(color = "Strategy: ", fill = "Strategy: ", x = "Erasure level",
       y = "Download time (seconds)") +
  facet_wrap(~ size, scales = "free_y") +
  scale_color_manual(values = c("plum3", "steelblue", "goldenrod")) +
  scale_fill_manual(values = c("plum3", "steelblue", "goldenrod")) +
  theme_bw() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 40, vjust = 0.75, hjust = 0.6))
```

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 6
#| fig-cap: As @fig-erasure-box-pre, but for the post-release experiment.
#| label: fig-erasure-box-post


dat |>
  filter(dataset == "Post-release") |>
  mutate(strategy = ifelse(strategy != "RACE", "NONE/DATA", "RACE")) |>
  mutate(size = case_match(size_kb, 1~"1 KB", 10~"10 KB", 100~"100 KB",
                           1000~"1 MB", 10000~"10 MB", 50000~"50 MB")) |>
  mutate(size = as_factor(size)) |>
  select(size, size_kb, strategy, server, erasure, time_sec) |>
  arrange(size, strategy, erasure, server) |>
  mutate(strategy = str_remove(strategy, "NONE/")) |>
  mutate(strategy = as_factor(ifelse(erasure == "NONE", "NONE", strategy))) |>
  mutate(size = as_factor(str_c("Size: ", size))) |>
  # Remove extreme outliers, because they otherwise distort the plot:
  filter(time_sec < 1.5 * IQR(time_sec) + quantile(time_sec, 0.75),
         .by = c(size, strategy, erasure)) |>
  ggplot(aes(x = erasure, y = time_sec, color = strategy, fill = strategy)) +
  geom_boxplot(alpha = 0.3, coef = Inf) +
  labs(color = "Strategy: ", fill = "Strategy: ", x = "Erasure level",
       y = "Download time (seconds)") +
  facet_wrap(~ size, scales = "free_y") +
  scale_color_manual(values = c("plum3", "steelblue", "goldenrod")) +
  scale_fill_manual(values = c("plum3", "steelblue", "goldenrod")) +
  theme_bw() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 40, vjust = 0.75, hjust = 0.6))
```



## Upload times
