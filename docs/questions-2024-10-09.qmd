---
title: "A first look at the data"
author: "Gyuri Barab√°s"
format:
  html:
    theme: cosmo
    embed-resources: true
knitr:
  opts_chunk:
    echo: true
    error: false
    warning: false
    message: false
latex-tinytex: false
---


### Data reading and formatting

Reading & formatting the data:

```{r}
library(tidyverse)
library(knitr)

dat <- jsonlite::fromJSON("../data/results-no50.json") |>
  tibble() |>
  rename(json = 1) |>
  mutate(storage = c("Swarm", "IPFS", "Arweave"), .before = 1) |>
  unnest(json) |>
  unnest(json) |>
  relocate(size, .after = storage) |>
  rename(time = download_time_seconds)

dat |> head(n = 10) |> knitr::kable()
```

Can we revert to data formatting of the test run if possible and convenient? It was more robust and easier to work with than the above.


### The `sha256_match` column

```{r}
dat |> count(sha256_match) |> kable()
```

Are `true` and `NA` (`null` in the original JSON) the only viable options here in principle?


### IPFS data are missing

```{r}
dat |> filter(storage == "IPFS") |> head(n = 10) |> kable()
```

Always 60 attempts and `sha256_match` is `NA`:

```{r}
dat |> filter(storage == "IPFS") |> count(attempts, sha256_match) |> kable()
```


### Faulty SWARM data point

```{r}
dat |> filter(storage == "Swarm") |> filter(is.na(sha256_match)) |> kable()
```

There was one single faulty data point for Swarm, with 15 attempts and no sha256_match. Do we know why?


### Arweave attempts

Arweave attempts slightly variable, though always with success in the end:

```{r}
dat |> filter(storage == "Arweave") |> count(attempts, sha256_match) |> kable()
```


### Gateway vs. node distinction

Is it based on `server` or `ip`? And how are servers / ip addresses organized more generally? In the data, there are 14 servers, 9 ip addresses, and 4 unique latitude-longitude combinations:

```{r}
dat |> count(server) |> arrange(n) |> kable()
dat |> count(ip) |> arrange(n) |> kable()
dat |> count(latitude, longitude) |> arrange(n) |> kable()
```

How should one interpret them? And: Is the experiment set up to be factorial in the servers? Currently the number of data points per server has a large spread. In fact, Andy wrote earlier today:

> So far I had one comment on your experimental design. I'm not sure I agree that controlling for file contents, i.e. using the same exact bitstring for each I/O size, is a good idea. While I can imagine an argument that upload speeds could be affected by inline compression, randomly generated files should be incompressible so I don't see how the actual contents can bias anything. Meanwhile, because of content addressing, using the same file for repeated downloads means that it's impossible to control for caching across different experimental runs. The only way to run the experiment "Upload 1MiB file, allow 1 day for propagation, then download without caching" multiple times is to do it with different files, since it's impossible to know when caches are cleared at every part of the data retrieval supply chain.

Our design, in principle, has 30 replicates per parameterization - do we actually have that implemented, and does that resolve the above concern?
