This is a quick description of the project on comparing data download speeds across three platforms: Swarm, Arweave, and Ipfs. The idea is to first upload files of various sizes to all three platforms, and then to download them under different circumstances. Then, one can compare the times needed for data retrieval and see if some platforms allow quicker downloads than others.

We want to elevate this concept to the level of a proper, repeatable, well-designed digital experiment. It should be sufficiently rigorous that its results should convince anyone of its correctness. Here's a proposed outline. I think that four factors should be varied independently, in such a way that changing one factor only changes one single aspect of the experiment. The factors are:
- File size. We could have data of size 1KB, 10KB, 100KB, 1MB, and 10MB (so 5 different levels).
- Platform. It could be Swarm, Arweave, or Ipfs (3 levels).
- Server. The identity of which server one initiates the download from might influence download speeds. So maybe we could designate three or four different servers and use those consistently (so 3 or 4 levels; for the purposes of example, let us say we have 3 servers and therefore 3 levels).
- Finally, every single combination of the above four factors should be replicated multiple times. My initial instinct is to create 30 replicates per combination, to allow for proper statistics.
This design gives us (5 filesizes) x (3 platforms) x (3 servers) x (30 replicates) = 1350 unique download experiments.

Some further points to keep in mind:
- The files should be randomly generated (with the unix program dd for example), have sizes exactly equal to the ones listed above (i.e., 1KB, 10KB, ..., 10MB), and, most importantly, there should only be one single unique file used per size and replicate. That way we know that we are uploading the exact same file when altering some other factor (e.g., changing the platform from Swarm to Ipfs), so changes in the file will not confound the results.
- For Swarm (and only Swarm), upload speed should also be measured. To do so, we should rely on the tags API to make sure that all chunks have been properly placed and uploaded to the system. When all chunks are placed, that is when upload has properly ended.
- Every download should be performed twice in a row. This is because the second download will then happen in the presence of caching (if supported by the target platform). So this way we are testing download speeds both with- and without caching. We of course expect downloads to be much faster with caching.
- Downloading should not start until after the system has properly stored the data. Since our files are small (up to 10MB), uploading should be done in about 10 minutes at most. So one crude but reliable way of doing the downloads is to wait exactly 1 hour after every upload, and only then begin downloading. This should give us more than enough time so that syncing issues are not a problem.
