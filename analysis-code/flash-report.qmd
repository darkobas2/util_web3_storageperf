---
title: "Benchmarking experiment flash report"
author: "György Barabás"
format:
  pdf:
    keep-tex: false
    fontsize: 11pt
    documentclass: article
    papersize: a4paper
    margin-left: 1in
    margin-right: 1in
    margin-top: 1in
    margin-bottom: 1in
    code-block-border-left: false
    number-sections: true
knitr:
  opts_chunk:
    eval: true
    echo: false
    message: false
    warning: false
    fig-align: center
execute: 
  cache: true
editor_options: 
  chunk_output_type: console
latex-tinytex: false
---



```{r}
library(lme4)
library(jsonlite)
library(tidyverse)
library(ggbeeswarm)
library(knitr)


dat <- read_rds("../data/compiled-data-new.rds")
```



## Empirical results

The raw results for all first-download data from the second repeat of the benchmarking experiment are in @fig-empirical. Here we have a grid of panels, with 7 rows and 3 columns. The columns are three different servers used for downloading. The rows indicate storage platform and level of erasure coding (if applicable). Within each panel, file size is along the x-axis and retrieval time along the y-axis, both on the log scale. Colors show retrieval strategies: when erasure coding is absent, blue means NONE, whereas in the presence of erasure coding, it means DATA. Yellow indicates the RACE strategy. Each point is a single download event, and the points have been arranged to reflect the general shape of their distribution.

The main take-aways from this figure are:

-   Arweave's download times increase the slowest, though it also takes longer to download small files from it.
-   IPFS download times increase somewhat faster.
-   Swarm increases the fastest, so for large files it is the least efficient. This was expected, given its underlying DISC model.
-   The level of erasure coding does not appear to have much of an effect on download speeds.
-   The DATA and RACE retrieval strategies do lead to differences. For small files RACE is faster; for larger files, sometimes RACE is faster, sometimes DATA.

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 10.5
#| fig-cap: Empirical results from the benchmarking experiment
#| label: fig-empirical
dat |>
  mutate(strategy = ifelse(strategy != "RACE", "NONE/DATA", "RACE")) |>
  mutate(pler = as_factor(case_when(
    platform != "Swarm" ~ platform,
    TRUE                ~ str_c(platform, ", ", erasure)
  ))) |>
  ggplot(aes(x = as_factor(size), y = time_sec, color = strategy, fill = strategy)) +
  #geom_quasirandom(alpha = 0.5) +
  geom_boxplot(alpha = 0.3, outlier.shape = NA) +
  scale_x_discrete(labels = c("1KB", "10KB", "100KB", "1MB", "10MB", "100MB")) +
  scale_y_log10() +
  scale_color_manual(values = c("steelblue", "goldenrod")) +
  scale_fill_manual(values = c("steelblue", "goldenrod")) +
  labs(x = "File size", y = "Download time (seconds)",
       color = "Strategy: ", fill = "Strategy: ") +
  #guides(color = guide_legend(override.aes = list(alpha = 1))) +
  facet_grid(pler ~ server) +
  theme_bw() +
  theme(legend.position = "bottom")
```



## Modeling download times

@fig-model shows model-predicted extrapolations made from the data. As seen, RACE is on average not better than DATA for large files (the dashed lines lie above the solid ones for files over 1GB). The models used in this figure are always generalized linear mixed models, but their structure is different for the three platforms.


### Arweave and IPFS

For Arweave and IPFS, the model reads

```
  time ~ log(size)^3 + (1 + log(size)^3 | server)
```

with a Gaussian family and a log-link function, where `time` is the download time in seconds and `size` is the file size in kilobytes. The log-link function means we are effectively estimating $\log(\text{time})$ as a function of $\log^3(\text{size})$, with a random intercept and slope provided by the identity of the server on which the downloads were performed.


### Swarm

Since Swarm additionally has erasure coding and different retrieval strategies, the fitted model is also more complex, although it is still a Gaussian generalized linear mixed model with a log-link function of the form

```
  time ~ log(size)^2 + erasure + strategy + log(size)^2:erasure
       + log(size)^2:strategy + erasure:strategy
       + (1 + log(size)^2 + erasure | server)
```

In words, we predict $\log(\text{time})$ using the main effects of $\log^2(\text{size})$, erasure level, and strategy, and all their possible two-way interactions. Plus we add a random intercept and slope (for both $\log^2(\text{size})$ and erasure level) based on server identity. In this model, erasure level is not a factor but a covariate reflecting the corresponding baseline probability of a faulty chunk retrieval event: it is taken as 0 for no erasure coding, as 1 for MEDIUM, 2 for STRONG, 3 for INSANE, and 4 for PARANOID. (Model selection indicated this as the best choice among a few other candidates, such as using the probability of individual chunk failure or using the fraction of chunks that are parity chunks.)

```{r}
#| out-width: 75%
#| fig-width: 6
#| fig-height: 4
#| fig-cap: Model-based predictions for mean file retrieval times. Here random effects are averaged over and therefore server identity no longer plays a role.
#| label: fig-model
modelSwarm <-
  dat |>
  filter(platform == "Swarm") |>
  mutate(strategy = as_factor(ifelse(strategy != "RACE", "NONE/DATA", "RACE"))) |>
  mutate(erasure = case_match(
    erasure,
    "NONE"     ~ 0,
    "MEDIUM"   ~ 1,
    "STRONG"   ~ 2,
    "INSANE"   ~ 3,
    "PARANOID" ~ 4
  )) |>
  glmer(time_sec ~ I(log(size)^2) + erasure + strategy +
          I(log(size)^2):erasure + I(log(size)^2):strategy +
          erasure:strategy + (1 + I(log(size)^2) + erasure | server),
        data = _, family = gaussian(link = "log"))

modelIPFS <-
  dat |>
  filter(platform == "IPFS") |>
  glmer(time_sec ~ I(log(size)^3) + (1 + I(log(size)^3) | server),
        data = _, family = gaussian(link = "log"))

modelArweave <-
  dat |>
  filter(platform == "Arweave") |>
  glmer(time_sec ~ I(log(size)^3) + (1 + I(log(size)^3) | server),
        data = _, family = gaussian(link = "log"))

dat |>
  mutate(strategy = as_factor(ifelse(strategy != "RACE", "NONE/DATA", "RACE"))) |>
  mutate(erasure_text = erasure, .after = erasure) |>
  mutate(erasure = case_match(
    erasure,
    "NONE"     ~ 0,
    "MEDIUM"   ~ 0.01,
    "STRONG"   ~ 0.05,
    "INSANE"   ~ 0.1,
    "PARANOID" ~ 0.5
  )) |>
  distinct(platform, erasure, erasure_text, strategy) |>
  crossing(size = 10^seq(log10(1), log10(1e7), l = 201)) |>
  (\(x) mutate(x, pred = case_when(
    platform == "Arweave" ~ predict(modelArweave, x, re.form = NA, type = "response"),
    platform == "IPFS" ~ predict(modelIPFS, x, re.form = NA, type = "response"),
    platform == "Swarm" ~ predict(modelSwarm, x, re.form = NA, type = "response")
  )))() |>
  mutate(erasure = as_factor(erasure_text), .keep = "unused") |>
  ggplot(aes(x = size, y = pred, color = erasure, linetype = strategy,
             group = str_c(platform, erasure, strategy))) +
  geom_line() +
  scale_x_log10(breaks = c(1e1, 1e3, 1e5, 1e7),
                labels = c("10 KB", "1 MB", "100 MB", "10 GB")) +
  scale_y_log10(breaks = c(1, 60, 3600, 86400),
                labels = c("1s", "1m", "1h", "1d")) +
  scale_color_viridis_d(option = "C", end = 0.8) +
  annotate(geom = "text", label = "Arweave", x = 8, y = 2.5) +
  annotate(geom = "text", label = "IPFS", x = 3000, y = 0.3) +
  annotate(geom = "text", label = "Swarm", x = 12000, y = 130) +
  labs(x = "File size", y = "Predicted download time",
       color = "Erasure:", linetype = "Strategy:") +
  theme_bw()
```


### Some further remarks

-   The modeled curves for Arweave and IPFS eventually increase faster than the curves for Swarm. This would mean that for very large files, Swarm becomes the most efficient platform. This is definitely an artifact and reflects the limitations of the models.
-   For large files, the RACE curves are slightly above DATA for Swarm. But since the y-axis is on the log scale, the actual difference is substantial. For example: for 10 GB files, the DATA strategy takes around 8 hours and the RACE strategy around 18 hours to download, with erasure levels playing only a minor role in shaping these figures.
