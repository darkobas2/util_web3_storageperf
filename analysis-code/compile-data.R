# Path to folder with the experiment whose results we are putting in a tidy table:
clargs <- commandArgs(trailingOnly = TRUE)
if (length(clargs) > 0) path <- clargs[1] else
  stop("Please provide a path to the current experiment's subdirectory.")


library(jsonlite) # Importing JSON files and making them into tables
library(tidyverse) # Efficient data manipulation and plotting
library(fs) # Easy interaction with the file system



# Load server ip addresses from a designated config file, and arrange
# them in a table. The table will have three columns: `server` (whether
# we are storing the ip of Server 1, Server 2, etc.), `platform` (Arweave,
# IPFS, or Swarm), and `ip` (the actual ip address).
serversFromConfig <- function(configFile) {
  # Read JSON data from file and convert to data frame:
  fromJSON(configFile) |>
    # Convert data frame to a tibble:
    as_tibble() |>
    # Choose only the three columns pertaining to the servers:
    select(contains("dl")) |>
    # Label them as "Server 1", "Server 2", ...:
    mutate(server = str_c("Server ", 1:3), .before = 1) |>
    # Simplify column names by dropping the "_dl_servers" suffix:
    rename_with(\(x) str_remove(x, "_dl_servers"), !server) |>
    # Tidy the data:
    pivot_longer(!server, names_to = "platform", values_to = "ip") |>
    # Change storage platform names to reflect proper capitalization:
    mutate(platform = case_match(
      platform,
      "arw"   ~ "Arweave",
      "ipfs"  ~ "IPFS",
      "swarm" ~ "Swarm"
    ))
}


# Load result data of the benchmarking experiment from a JSON file,
# and arrange them in a rectangular table:
dataFromJsonRaw <- function(jsonFile) {
  # Read JSON data file:
  fromJSON(jsonFile) |>
    # Convert to a tibble:
    as_tibble() |>
    # Unpack the nested `tests` column:
    unnest(tests) |>
    # And then the sub-nested `results` column:
    unnest(results) |>
    # Give new names to some of the columns:
    rename(time_sec = download_time_seconds,
           replicate = ref,
           platform = storage)
}


# Take the raw data generated by dataFromJsonRaw(), and tidy it up:
dataFromJson <- function(rawTable, configFile) {
  # Start from the tibble generated by dataFromJsonRaw():
  rawTable |>
    # Convert the JSON true/false into R's native TRUE and FALSE:
    mutate(sha256_match = (sha256_match == "true")) |>
    # Remove unnecessary columns (note: size_kb does not actually
    # measure file size; it should be dropped):
    select(!size_kb & !server & !timestamp) |>
    # Properly capitalize IPFS in the `platform` column - important
    # for matching with the server ip data from serversFromConfig():
    mutate(platform = ifelse(platform == "Ipfs", "IPFS", platform)) |>
    # Now join table with server ip info, so we'll know which ip
    # is Server 1, which is Server 2, and so on:
    semi_join(serversFromConfig(configFile), by = join_by(platform, ip)) |>
    left_join(serversFromConfig(configFile), by = join_by(platform, ip)) |>
    # Rearrange the order of the columns and rename some of them:
    relocate(size, server, time_sec, attempts, sha256_match, .after = platform) |>
    relocate(timeout = `dl_retrieval-timeout`,
             strategy = dl_redundancy,
             erasure = ul_redundancy,
             .after = time_sec) |>
    # Give the different erasure coding levels and retrieval strategies
    # human-readable names:
    mutate(erasure = case_match(
      erasure,
      0  ~ "NONE",
      1  ~ "MEDIUM",
      2  ~ "STRONG",
      3  ~ "INSANE",
      4  ~ "PARANOID",
      NA ~ "NONE")
    ) |>
    mutate(strategy = case_match(
      strategy,
      0 ~ "NONE",
      1 ~ "DATA",
      3 ~ "RACE")
    ) |>
    # File size is a character string; convert to integer:
    mutate(size = as.integer(size)) |>
    # Indicate units in column name:
    rename(size_kb = size)
}


# Convenience function for simultaneously loading and cleaning the data:
prepareData <- function(jsonFile, configFile) {
  dataFromJson(dataFromJsonRaw(jsonFile), configFile)
}



# Load data from original JSON file(s), and convert to a tidy table:
dat <-
  # List of JSON file(s) with download benchmark data:
  tibble(file = Sys.glob(path(path, "results_onlyswarm*.json"))) |>
  # Configuration file (also in JSON format):
  mutate(conf = path(path, "config.json")) |>
  # Using these, load the data and put them in a nested table:
  mutate(data = map2(file, conf, prepareData)) |>
  # Unnest the data:
  unnest(data) |>
  # Only retain necessary columns:
  select(platform, size_kb, server, erasure, strategy,
         time_sec, sha256_match, attempts)


# Some simple checks, to gauge data integrity:
cat(str_c("\n\nAny failed downloads (number of files failing the sha256 match):\n\n"))
dat |> count(sha256_match)

cat(str_c("\n\nDistribution of the number of download attempts ",
          "(max 15 before giving up):\n\n"))
dat |> count(attempts)

cat(str_c("\n\nDistribution of the number of download attempts, conditional on success ",
          "(i.e., those that did succeed in at most 15 attempts):\n\n"))
dat |> filter(sha256_match) |> count(attempts)

cat(str_c("\n\nAny weird data points where strategy = NONE despite erasure coding ",
          "(should be none)?\n\n"))
dat |>
  filter(erasure != "NONE" & strategy == "NONE") |>
  (\(x) if (nrow(x) == 0) cat("none\n") else print(x, n = Inf))()

cat(str_c("\n\nNumber of replicates per factor combination ",
          "(should be 30 everywhere):\n\n"))
dat |>
  count(platform, server, size_kb, erasure, strategy) |>
  print(n = Inf)


# Save data in compressed rds format, using the specified path:
write_rds(dat, path(path, "swarm.rds"), compress = "xz")
