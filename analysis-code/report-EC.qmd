---
title: "Analysis of first run of the benchmarking experiment"
author: "György Barabás"
format:
  html:
    theme: cosmo
    number-sections: false
    embed-resources: true
    code-fold: true
knitr:
  opts_chunk:
    eval: true
    echo: true
    message: false
    warning: false
    fig-align: center
execute: 
  cache: false
editor_options: 
  chunk_output_type: console
latex-tinytex: false
---



## Preliminary data analysis

We first load some packages and set up some functions to read and tidy the raw data:

```{r}
library(knitr) # Neatly formatted tables (for output)
library(jsonlite) # Converting JSON files to data frames
library(tidyverse) # Efficient data manipulation and plotting
library(broom) # Tabular formatting of statistical model outputs
library(ggfortify) # Diagnostic plots for linear statistical models



# Load server ip addresses from a designated config file, arranged in
# a table. The table will have three columns: `server` (whether we
# are storing the ip of Server 1, Server 2, etc.), `platform` (Swarm,
# IPFS, or Arweave), and `ip` (the actual ip address).
serversFromConfig <- function(configFile = "../data/config.json") {
  # Read JSON data from file and convert to data frame:
  fromJSON(configFile) |>
    # Convert data frame to a tibble:
    as_tibble() |>
    # Choose only the three columns pertaining to the servers:
    select(contains("dl")) |>
    # Label them as "Server 1", "Server 2", ...:
    mutate(server = str_c("Server ", 1:3), .before = 1) |>
    # Simplify column names by dropping the "_dl_servers" suffix:
    rename_with(\(x) str_remove(x, "_dl_servers"), !server) |>
    # Tidy the data:
    pivot_longer(!server, names_to = "platform", values_to = "ip") |>
    # Change storage platform names to reflect proper capitalization:
    mutate(platform = case_match(
      platform,
      "swarm" ~ "Swarm",
      "ipfs"  ~ "IPFS",
      "arw"   ~ "Arweave"
    ))
}


# Load result data of the benchmarking experiment from a JSON file,
# and arrange them in a rectangular table:
dataFromJsonRaw <- function(jsonFile = "../results.json") {
  # Read JSON data file:
  fromJSON(jsonFile) |>
    # Convert to a tibble:
    as_tibble() |>
    # Unpack the nested `tests` column:
    unnest(tests) |>
    # And then the sub-nested `results` column:
    unnest(results) |>
    # Give new names to some of the columns:
    rename(time_sec = download_time_seconds,
           replicate = ref,
           platform = storage)
}


# Take the raw data generated by dataFromJsonRaw(), and tidy it up:
dataFromJson <- function(rawTable) {
   # Start from the tibble generated by dataFromJsonRaw():
  rawTable |>
    # Convert the JSON true/false into R's native TRUE and FALSE:
    mutate(sha256_match = (sha256_match == "true")) |>
    # Remove unnecessary columns:
    select(!size_kb & !server & !timestamp) |>
    # Properly capitalize IPFS in the `platform` column - important
    # for matching with the server ip data from serversFromConfig():
    mutate(platform = ifelse(platform=="Ipfs", "IPFS", platform)) |>
    # Now join table with server ip info, so we'll know which ip
    # is Server 1, which is Server 2, and so on:
    semi_join(serversFromConfig(), by = join_by(platform, ip)) |>
    left_join(serversFromConfig(), by = join_by(platform, ip)) |>
    # Rearrange the order of the columns and rename some of them:
    relocate(size, server, time_sec, attempts, sha256_match, .after = platform) |>
    relocate(timeout = `dl_retrieval-timeout`,
             strategy = dl_redundancy,
             erasure = ul_redundancy,
             .after = time_sec) |>
    # Give the different erasure coding levels and
    # retrieval strategies human-readable names:
    mutate(erasure = case_match(
      erasure,
      0 ~ "NONE",
      1 ~ "MEDIUM",
      2 ~ "STRONG",
      3 ~ "INSANE",
      4 ~ "PARANOID")
    ) |>
    mutate(strategy = case_match(
      strategy,
      0 ~ "NONE",
      1 ~ "DATA",
      3 ~ "RACE")
    ) |>
    # File size is a character string; convert to integer:
    mutate(size = as.integer(size))
}


# Convenience function to simultaneously load and clean the data:
prepareData <- function(jsonFile) {
  dataFromJson(dataFromJsonRaw(jsonFile))
}
```

The data are stored in two files, which we load and then merge into one master dataset:

```{r}
dat <-
  # Glue together the rows of the two data sources:
  bind_rows(
    prepareData("../data/results_onlyswarm_2024-11-06_19-08.json"),
    prepareData("../data/results_onlyswarm_2024-11-07_19-24.json")
  ) |>
  # Choose only relevant columns:
  select(platform, size, server, erasure, strategy,
         time_sec, sha256_match, attempts)
```

Let us do a couple of quick sanity checks to make sure that the data look reasonable. First we check if there are any failed download attempts:

```{r}
dat |>
  count(sha256_match, attempts) |>
  kable()
```

There are 20 failures; each took 15 attempts before failing. Otherwise, all other downloads succeeded on the first attempt. Let us see where these failures are:

```{r}
dat |>
  filter(!sha256_match) |>
  select(size, server, erasure, strategy) |>
  kable()
```

So 15 downloads failed for 1000 KB (1 MB) files with no erasure coding, two failed for 100 MB files with MEDIUM erasure coding and the DATA retrieval strategy, and three more failed for 100 MB files with INSANE erasure coding and the DATA retrieval strategy. Overall, a very small number of attempted downloads failed, so we can disregard this problem and proceed simply by filtering out the failed samples from the data.

After that, let us check whether the experiment is well balanced:

```{r}
dat |>
  # Keep only those rows where the download succeeded:
  filter(sha256_match) |>
  count(platform, server, size, erasure, strategy, name = "samples") |>
  kable()
```

For the overwhelming majority of factor combinations, we have 35 observations -- the design is well balanced. Some differences can be observed (7 out of the 90 combinations do not have 35 samples), but they are few and far between, and largely caused by the failed downloads:

```{r}
dat |>
  # Keep only those rows where the download succeeded:
  filter(sha256_match) |>
  count(platform, server, size, erasure, strategy, name = "samples") |>
  count(samples) |>
  kable()
```



## Visualizing the data

We can make a plot of the results:

```{r}
#| fig-width: 8
#| fig-height: 6
dat |>
  # Keep only those rows where the download succeeded:
  filter(sha256_match) |>
  # Reorder factors to show up better in plot:
  mutate(erasure = fct_relevel(erasure,
                               "NONE", "MEDIUM", "INSANE")) |>
  mutate(erasure = fct_relabel(erasure,
                               \(x) str_c("Erasure level: ", x))) |>
  mutate(strategy = fct_relevel(strategy, "NONE", "DATA", "RACE")) |>
  # Create plot:
  ggplot(aes(x = as_factor(size), y = time_sec,
             color = strategy, fill = strategy)) +
  geom_boxplot(alpha = 0.3, coef = Inf) +
  # Human-readable file size labels along the x-axis:
  scale_x_discrete(labels = c("1KB", "10KB", "100KB",
                              "1MB", "10MB", "100MB")) +
  scale_y_log10() +
  scale_color_manual(values = c(NONE = "steelblue",
                                DATA = "goldenrod",
                                RACE = "forestgreen")) +
  scale_fill_manual(values  = c(NONE = "steelblue",
                                DATA = "goldenrod",
                                RACE = "forestgreen")) +
  labs(x = "File size", y = "Download time (seconds)",
       color = "Retrieval strategy:", fill = "Retrieval strategy:") +
  facet_grid(server ~ erasure) +
  theme_bw() +
  theme(legend.position = "bottom")
```

Here we see a 3x3 grid of facets. The rows correspond to the three different servers. The columns represent the various erasure coding levels. The colors differentiate three retrieval strategies. The ordinate has the download time, in seconds and on the log scale, and the abscissa the file size -- also on the log scale. The box plots are standard (box: middle half of the data; top whisker: top quartile; bottom whisker: bottom quartile; thick line inside the box: median), except the whiskers span the full range of the data instead of explicitly showing outliers.

Note that when erasure level is NONE, there are no additional retrieval strategies. This is because without erasure coding, those various strategies do not make any difference in the first place. Therefore, in the first column only the retrieval strategy of NONE is seen.

An alternative visualization plots the actual data points themselves, with a small sideways artificial jitter to reduce overplotting:

```{r}
#| fig-width: 8
#| fig-height: 6
dat |>
  # Keep only those rows where the download succeeded:
  filter(sha256_match) |>
  # Reorder factors to show up better in plot:
  mutate(erasure = fct_relevel(erasure,
                               "NONE", "MEDIUM", "INSANE")) |>
  mutate(erasure = fct_relabel(erasure,
                               \(x) str_c("Erasure level: ", x))) |>
  mutate(strategy = fct_relevel(strategy, "NONE", "DATA", "RACE")) |>
  # Create plot:
  ggplot(aes(x = as_factor(size), y = time_sec, color = strategy)) +
  geom_point(alpha = 0.5, shape = 1,
             position = position_jitterdodge(jitter.width = 0.4)) +
  # Human-readable file size labels along the x-axis:
  scale_x_discrete(labels = c("1KB", "10KB", "100KB",
                              "1MB", "10MB", "100MB")) +
  scale_y_log10() +
  scale_color_manual(values = c(NONE = "steelblue",
                                DATA = "goldenrod",
                                RACE = "forestgreen")) +
  labs(x = "File size", y = "Download time (seconds)",
       color = "Retrieval strategy:") +
  guides(color = guide_legend(override.aes = list(alpha = 1))) +
  facet_grid(server ~ erasure) +
  theme_bw() +
  theme(legend.position = "bottom")
```

There is an accelerating increase of log download times with log file size. Erasure coding does seem to improve download speeds, especially with the RACE retrieval strategy. (For the DATA strategy, the speed increase is mostly at smaller files only.) It is difficult to say based on the figure whether the server's identity matters, even though there are some visual differences between them.



## Modeling the data

We now turn to building a simple predictive model for these data. Given the nonlinear increase in download times with file size, and based on the figures above, let us model this increase using a quadratic function. So all other things equal, we write
$$
\log(t_i)
= \beta_0
+ \beta_1 \log^2(s_i)
+ \epsilon_i ,
$$ {#eq-model-simp}
where $t_i$ and $s_i$ are respectively the $i$th download time and file size in the data, $\beta_0$ is an intercept, $\beta_1$ is a slope, and $\epsilon_i$ is the $i$th residual.

Apart from $\log^2(s_i)$, a continuous predictor, we also consider server identity, erasure coding, and retrieval strategy as factors. In fact, we will merge the latter two into a single factor, with levels `NONE_NONE`, `MEDIUM_DATA`, `MEDIUM_RACE`, `INSANE_DATA`, and `INSANE_RACE`. This is needed due to the fact that when erasure level is NONE, it makes no sense to implement any retrieval strategy other than NONE. Similarly, when erasure coding is anything but NONE, it makes no sense to have a strategy that is NONE.

The expanded model thus reads:
$$
\begin{aligned}
\log(t_i)
&= \beta_0
+ \beta_1 \log^2(s_i)
+ \beta_2 \text{(Server 2)}_i
+ \beta_3 \text{(Server 3)}_i
\\ & \quad
+ \beta_4 \text{(MEDIUM\_DATA)}_i
+ \beta_5 \text{(MEDIUM\_RACE)}_i
\\ & \quad
+ \beta_6 \text{(INSANE\_DATA)}_i
+ \beta_7 \text{(INSANE\_RACE)}_i
+ \epsilon_i ,
\end{aligned}
$$ {#eq-model}
where $\text{(Server 2)}_i$, $\text{(Server 3)}_i$, $\text{(MEDIUM\_DATA)}_i$ etc. are indicator variables that take on the value 1 if data point $i$ falls in that category and 0 otherwise. The baseline factor levels are Server 1 and `NONE_NONE`, so the coefficients $\beta_2$ to $\beta_7$ provide treatment contrasts against this baseline.

This is a linear model in the coefficients $\beta_i$, with both discrete and continuous predictors. This is therefore an ANCOVA problem, and we can proceed via multiple regression:

```{r}
jointModel <-
  dat |>
  # Keep only those rows where the download succeeded:
  filter(sha256_match) |>
  # Merge erasure and strategy into a single variable:
  mutate(erasure_strategy = str_c(erasure, strategy, sep = "_")) |>
  # Specify its factor levels, making sure that NONE_NONE is first:
  mutate(erasure_strategy = fct_relevel(
    erasure_strategy, "NONE_NONE", "MEDIUM_DATA",
    "MEDIUM_RACE", "INSANE_DATA", "INSANE_RACE"
  )) |>
  # Put both the predictor and the response on the log scale:
  mutate(log_size = log(size), log_time = log(time_sec)) |>
  # Select relevant columns only:
  select(server | erasure_strategy | log_size | log_time) |>
  # Perform the linear regression:
  lm(log_time ~ I(log_size^2) + server + erasure_strategy, data = _)
```

Let us explore this model. First, before interpreting the regression outcome, we must make sure that the assumptions of linear regression were fulfilled via diagnostic plots:

```{r}
#| fig-width: 8
#| fig-height: 6
jointModel |>
  autoplot(smooth.colour = NA, colour = "steelblue",
           alpha = 0.3, shape = 1, which = 1:3) +
  theme_bw()
```

Independence of residuals (bottom left) largely holds, although a closer look reveals some micro-structure where certain point segments steadily go downwards. But overall, this plot looks good, and so does the bottom left one (showing that the residual variances are homoscedastic). Finally, the quantile-quantile plot is not beautiful at the lower end, but is otherwise well-behaved. This discrepancy affects relatively few data point only, and is due to the variation in download speeds for 100 MB files under INSANE erasure coding and the RACE strategy (see above). We can also plot the residuals using a histogram, which further shows that apart from a few points in the lower tail, the residuals are normally distributed:

```{r}
tibble(residuals = residuals(jointModel)) |>
  ggplot(aes(x = residuals)) +
  geom_histogram(color = "steelblue", fill = "steelblue", alpha = 0.2, bins = 30) +
  theme_bw()
```

This means that we can use our model to make predictions and inference. Let us look at the ANOVA table:

```{r}
jointModel |>
  anova() |>
  tidy() |>
  kable()
```

All predictors come out as significant, to the point that the p-values do not even show up (they are all below $2.1\cdot 10^{-47}$, in two cases substantially so). With that, we can look at the regression table:

```{r}
jointModel |>
  summary() |>
  tidy() |>
  mutate(term = str_remove(term, "server")) |>
  mutate(term = str_remove(term, "erasure_strategy")) |>
  kable()
```

Not only have all coefficients very low p-values, the standard errors are also always low compared with their estimated values. The rows of this table correspond to $\beta_0$, $\beta_1$, ..., $\beta_7$, in that order.

To see how well the model predicts the data, let us plot the raw data (points) together with the model predictions (lines):

```{r}
#| fig-width: 8
#| fig-height: 6
dat |>
  # Keep only those rows where the download succeeded:
  filter(sha256_match) |>
  # Attach predictions to data table:
  mutate(pred = exp(predict(jointModel))) |>
  mutate(erasure = fct_relevel(erasure,
                               "NONE", "MEDIUM", "INSANE")) |>
  mutate(erasure = fct_relabel(erasure,
                               \(x) str_c("Erasure level: ", x))) |>
  mutate(strategy = fct_relevel(strategy, "NONE", "DATA", "RACE")) |>
  ggplot(aes(x = size, color = strategy, fill = strategy)) +
  geom_point(aes(y = time_sec), alpha = 0.5, shape = 1,
             position = position_jitterdodge(jitter.width = 0.5)) +
  geom_line(aes(y = pred), linewidth = 1) +
  scale_x_log10(breaks = 10^(0:5),
                labels = c("1KB", "10KB", "100KB",
                           "1MB", "10MB", "100MB")) +
  scale_y_log10() +
  scale_color_manual(values = c(NONE = "steelblue",
                                DATA = "goldenrod",
                                RACE = "forestgreen")) +
  scale_fill_manual(values  = c(NONE = "steelblue",
                                DATA = "goldenrod",
                                RACE = "forestgreen")) +
  labs(x = "File size", y = "Download time (seconds)",
       color = "Retrieval strategy:", fill = "Retrieval strategy:") +
  facet_grid(server ~ erasure) +
  theme_bw() +
  theme(legend.position = "bottom")
```

